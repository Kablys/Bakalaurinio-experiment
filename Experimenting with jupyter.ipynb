{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine artcile files into one file\n",
    "from os import listdir\n",
    "allData = []\n",
    "for filename in listdir('articles'):\n",
    "    with open(\"articles/\" + filename, \"r\") as read_file:\n",
    "        allData.append(json.load(read_file))\n",
    "        \n",
    "with open(\"delfi_all.json\", \"w\") as write_file:\n",
    "    json.dump(allData, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">TfidfVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(4058, 93)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Do document clustering experiment.')\n",
    "parser.add_argument(\"--raw\",      action='store_true')\n",
    "parser.add_argument(\"--title\",    action='store_true')\n",
    "parser.add_argument(\"--intro\",    action='store_true')\n",
    "parser.add_argument(\"--stopword\", action='store_true')\n",
    "parser.add_argument(\"--stemer\",   action='store_true')\n",
    "parser.add_argument(\"--lemstop\",  action='store_true')\n",
    "parser.add_argument(\"--lemmer\",   action='store_true')\n",
    "\n",
    "parser.add_argument(\"--ngram\",  type=int,   choices=range(1, 7),)\n",
    "parser.add_argument(\"--min_df\", type=int,   choices=range(1, 11),)\n",
    "parser.add_argument(\"--max_df\", type=float,) \n",
    "parser.add_argument(\"--dr\",     type=int,   choices=range(1, 101),)\n",
    "\n",
    "parser.add_argument('--methods', nargs='+', choices='km em ac aa aw db'.split(),\n",
    "                    default='km em ac aa aw db'.split(),)\n",
    "args = parser.parse_args('--lemmer'.split())\n",
    "#args = parser.parse_args() \n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def make_dataset(experiment_data, **kwargs):\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    matrix = vectorizer.fit_transform(experiment_data)\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print('>' + str(vectorizer))\n",
    "    print(matrix.shape)\n",
    "    return [{\"matrix\" : matrix, \"names\" : names}]  \n",
    "\n",
    "data, all_tokens, morfs = ([],[],[])\n",
    "with open(\"delfi_all.json\", \"r\") as read_file:\n",
    "    allData = json.load(read_file)\n",
    "all_tokens = [\" \".join(d[\"tokens\"] + d[\"stop_tokens\"]) for d in allData]\n",
    "morfs = [re.findall(\"<word=\\\"(.*)\\\" lemma=\\\"(\\w*).*\\\" type=\\\"(.*)\\\"\", d[\"lemms\"]) for d in allData]\n",
    "\n",
    "datasets = []\n",
    "#args = parser.parse_args('--'.split()) # For testing\n",
    "\n",
    "datasets += make_dataset(all_tokens, analyzer = 'char_wb', ngram_range = (1,1))\n",
    "\n",
    "# if args.raw:  \n",
    "#     datasets += make_dataset(all_tokens)\n",
    "# if args.title:\n",
    "#     datasets += make_dataset([re.sub(\"[\\W\\d_]+\", \" \", d[\"title\"]).lower() for d in data])\n",
    "# if args.intro:\n",
    "#     datasets += make_dataset([re.sub(\"[\\W\\d_]+\", \" \", d[\"intro\"]).lower() for d in data])\n",
    "# if args.stopword:\n",
    "#     datasets += make_dataset([\" \".join(d[\"tokens\"])                    for d in data])\n",
    "# if args.stemer:\n",
    "#     datasets += make_dataset([\" \".join(d[\"stems\"]  + d[\"stop_stems\"])  for d in data])\n",
    "\n",
    "# if args.lemstop:\n",
    "#     datasets += make_dataset([\" \".join([l[0] for l in m if l[2]\n",
    "#                    .startswith((\"dkt\", \"vksm\", \"bdv\", , \"bendr\", \"būdn\", \"pad\", \"pusd\", \"tikr\"))]) for m in morfs])\n",
    "# if args.lemmer:\n",
    "#     datasets += make_dataset([\" \".join([l[1] for l in m]) for m in morfs])\n",
    "    \n",
    "# if args.ngram:\n",
    "#     n = args.ngram\n",
    "#     datasets += make_dataset(all_tokens, analyzer = 'char_wb', ngram_range = (n,n))\n",
    "# if args.min_df:\n",
    "#     df = args.min_df\n",
    "#     datasets += make_dataset(all_tokens, min_df = df)\n",
    "# if args.max_df:\n",
    "#     df = args.max_df\n",
    "#     datasets += make_dataset(all_tokens, max_df = df)\n",
    "# if args.dr:\n",
    "#     svd = TruncatedSVD(args.dr)\n",
    "#     lsa = make_pipeline(svd, Normalizer(copy=False)) \n",
    "#     X = make_dataset(all_tokens)[0]\n",
    "#     matrix = csr_matrix(lsa.fit_transform(X[\"matrix\"]))\n",
    "#     datasets += [{\"matrix\" : matrix, \"names\" : X[\"names\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4058"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_names = ['Auto', 'Veidai', 'Sportas', 'Mokslas', 'Verslas']\n",
    "categorys  = np.array([category_names.index(d[\"categorys\"]) for d in data])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '²', '³', 'º', '¼', 'à', 'á', 'â', 'ä', 'å', 'ç', 'è', 'é', 'ë', 'ì', 'í', 'ï', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ü', 'ā', 'ą', 'č', 'ė', 'ę', 'ī', 'į', 'ļ', 'ł', 'ņ', 'ō', 'œ', 'ś', 'š', 'ū', 'ų', 'ž', 'ʼ', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ш', 'ы', 'э']\n"
     ]
    }
   ],
   "source": [
    "def for_table (stats):\n",
    "    np.unique(stats, return_counts=True)\n",
    "    for i in np.unique(stats, return_counts=True):\n",
    "        for j in i:\n",
    "            print(j, end=\"\\t\")\n",
    "        print()\n",
    "#     for i in np.unique(stats, return_counts=True)[1]: # Do this in spreedsheet\n",
    "#         print(i/len(stats)*100, end=\"\\t\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "        \n",
    "#MINI TESTS\n",
    "#\n",
    "#TAGS\n",
    "# dt = [len(d['tags']) for d in data]\n",
    "# print('Tag stats:')\n",
    "# print(\"Average amount of tags: {}\".format(np.average(dt)))\n",
    "# for i in np.unique(dt, return_counts=True):\n",
    "#     for j in i:\n",
    "#         print(j, end=\" \")\n",
    "#     print()\n",
    "# print(\"\".format(max))\n",
    "\n",
    "#PARTS OF SPEECH\n",
    "# morfs[0][1][2].split()[0]\n",
    "# ps=[]\n",
    "# for m in morfs:\n",
    "#     for w in m:\n",
    "#         word_type = re.split(\"\\W+\", w[2])[0]\n",
    "# #         if word_type == \"tikr\":\n",
    "# #             print(w)\n",
    "#         ps.append(word_type)\n",
    "# for_table(ps)\n",
    "# len(ps)\n",
    "\n",
    "#NGRAM MORE THEN 5\n",
    "# for n in range(6,11):\n",
    "#     print(n)\n",
    "#     vectorizer = TfidfVectorizer(analyzer = 'char_wb', ngram_range = (n,n))\n",
    "#     matrix = vectorizer.fit_transform(all_tokens)\n",
    "#     print(matrix.shape)\n",
    "\n",
    "#INPUT DATA STATS\n",
    "# raw = all_tokens\n",
    "# title = [re.sub(\"[\\W\\d_]+\", \" \", d[\"title\"]).lower() for d in data]\n",
    "# intro = [re.sub(\"[\\W\\d_]+\", \" \", d[\"intro\"]).lower() for d in data]\n",
    "# for i in [raw, title, intro]:\n",
    "#     lengths = [len(j.split()) for j in i]\n",
    "#     print(min(lengths), max(lengths), np.mean(lengths))\n",
    "\n",
    "#TOP LEMMAS\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# bin_vectorizer = CountVectorizer(binary = True)\n",
    "# %time bX = bin_vectorizer.fit_transform([\" \".join([l[1] for l in m]) for m in morfs])\n",
    "# print(bX.shape)\n",
    "# full_vectorizer = CountVectorizer()\n",
    "# %time fX = full_vectorizer.fit_transform([\" \".join([l[1] for l in m]) for m in morfs])\n",
    "# print(fX.shape)\n",
    "\n",
    "# # TODO max_df 0.9 - 0.1; min_df 2-10 (test if 1 make difference)\n",
    "\n",
    "# n_docs = bX.shape[0]\n",
    "# n_tokens = sum([len(d[\"tokens\"] + d[\"stop_tokens\"]) for d in data])\n",
    "# terms = bin_vectorizer.get_feature_names()\n",
    "# counts = bX.sum(axis=0).A1\n",
    "# total_counts = fX.sum(axis=0).A1\n",
    "# unique_counts = list(set(counts))\n",
    "# biggest = sorted(unique_counts)[::-1]\n",
    "# top = 0\n",
    "# stats = []\n",
    "# for i in biggest:\n",
    "#     term_indices = np.where(counts == i)[0]\n",
    "\n",
    "#     for j in term_indices:\n",
    "#             top += 1\n",
    "#             stats.append(\"{},{},{},{}%,{},{}%\".format(top,\n",
    "#                                         terms[j],\n",
    "#                                         #j, # index in terms\n",
    "#                                         total_counts[j],\n",
    "#                                         total_counts[j]/n_tokens*100,\n",
    "#                                         i, # count\n",
    "#                                         i/n_docs*100)) #percent of all docs\n",
    "\n",
    "# with open(\"lem_stats.csv\", \"w\") as word_stats:\n",
    "#     word_stats.write(\"\\n\".join(stats))\n",
    "\n",
    "#STEM, WORD, LEM LENGTHS\n",
    "#TODO test with stops, remove empty strings, test lems\n",
    "# stems = [d[\"stems\"] + d[\"stop_stems\"] for d in data]\n",
    "# flat_stems = [item for sublist in stems for item in sublist]\n",
    "# stems_lens = [len(stem) for stem in flat_stems if len(stem) > 0]\n",
    "# print(min(stems_lens), max(stems_lens), np.mean(stems_lens))\n",
    "# word_lens = []\n",
    "# for sub in all_tokens:\n",
    "#     for item in sub.split():\n",
    "#         word_lens.append(len(item))\n",
    "#     #len(item) for sublist in all_tokens for item in sublist]\n",
    "# print(min(word_lens), max(word_lens), np.mean(word_lens))\n",
    "# lem_lens = []\n",
    "# for sub in [\" \".join([l[1] for l in m]) for m in morfs]:\n",
    "#     for item in sub.split():\n",
    "#         lem_lens.append(len(item))\n",
    "# print(min(lem_lens), max(lem_lens), np.mean(lem_lens))\n",
    "\n",
    "# 1-GRAM LETTERS\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = 'char_wb', ngram_range = (1,1))\n",
    "matrix = vectorizer.fit_transform(all_tokens)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'K-means', 'model': KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=5, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)}, {'title': 'Expectation–maximization', 'model': GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
      "        means_init=None, n_components=5, n_init=1, precisions_init=None,\n",
      "        random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,\n",
      "        verbose_interval=10, warm_start=False, weights_init=None)}, {'title': 'Complete-linkage clustering', 'model': AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
      "            connectivity=None, linkage='complete', memory=None,\n",
      "            n_clusters=5, pooling_func='deprecated')}, {'title': 'Average-linkage clustering', 'model': AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
      "            connectivity=None, linkage='average', memory=None,\n",
      "            n_clusters=5, pooling_func='deprecated')}, {'title': 'Ward-linkage clustering', 'model': AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
      "            connectivity=None, linkage='ward', memory=None, n_clusters=5,\n",
      "            pooling_func='deprecated')}, {'title': 'DBSCAN', 'model': DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
      "    metric_params=None, min_samples=5, n_jobs=-1, p=None)}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "K = 5\n",
    "jobs = -1\n",
    "\n",
    "methods = []\n",
    "\n",
    "if \"km\" in args.methods:\n",
    "    KMtitle = \"K-means\"\n",
    "    KMmodel = KMeans(n_clusters=K,\n",
    "#                  max_iter=1,\n",
    "#                  n_init=1,\n",
    "                 n_jobs=jobs,\n",
    "                 random_state=42,)\n",
    "    methods += [{\"model\": KMmodel, \"title\": KMtitle}]\n",
    "    \n",
    "if \"em\" in args.methods:\n",
    "    EMtitle = \"Expectation–maximization\"\n",
    "    EMmodel = GaussianMixture(n_components=K,\n",
    "                              covariance_type='diag',\n",
    "#                         n_init=10,\n",
    "                              random_state=42,)\n",
    "    methods += [{\"model\": EMmodel, \"title\": EMtitle}]\n",
    "    \n",
    "if \"ac\" in args.methods:\n",
    "    ACtitle = \"Complete-linkage clustering\"\n",
    "    ACmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                      linkage='complete',)\n",
    "    methods += [{\"model\": ACmodel, \"title\": ACtitle}]\n",
    "    \n",
    "if \"aa\" in args.methods:\n",
    "    AAtitle = \"Average-linkage clustering\"\n",
    "    AAmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='average',)\n",
    "    methods += [{\"model\": AAmodel, \"title\": AAtitle}]\n",
    "    \n",
    "if \"aw\" in args.methods:\n",
    "    AWtitle = \"Ward-linkage clustering\"\n",
    "    AWmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='ward',)\n",
    "    methods += [{\"model\": AWmodel, \"title\": AWtitle}]\n",
    "    \n",
    "if \"db\" in args.methods:\n",
    "    DBSCANtitle = \"DBSCAN\"\n",
    "    DBSCANmodel = DBSCAN(n_jobs = jobs,)\n",
    "    methods += [{\"model\": DBSCANmodel, \"title\": DBSCANtitle}]\n",
    "print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import mode\n",
    "\n",
    "def print_top_terms(model, terms):\n",
    "#    print(\"Top terms per cluster:\")\n",
    "    centers = model.cluster_centers_ if isinstance(model, KMeans) else model.means_\n",
    "    order_centroids = centers.argsort()[:, ::-1]\n",
    "    for i in range(K):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n",
    "\n",
    "def get_new_labels(clusters):\n",
    "    new_labels = np.zeros_like(clusters)\n",
    "    print(\"New labels:\")\n",
    "    for i in range(K):\n",
    "        mask = (clusters == i)\n",
    "        closest_category = mode(categorys[mask])[0][0]\n",
    "        new_labels[mask] = closest_category\n",
    "        print(\"{} -> {}({})\".format(i, closest_category, category_names[closest_category]))\n",
    "    print(np.bincount(new_labels))\n",
    "    return new_labels\n",
    "\n",
    "def print_metrics(y_pred):\n",
    "    print(\"Rand        %.3f\" %(adjusted_rand_score(categorys, y_pred)))\n",
    "    print(\"Homogeneity %.3f\" %(homogeneity_score(categorys, y_pred)))\n",
    "    print(\"Completeness %.3f\" %(completeness_score(categorys, y_pred)))\n",
    "\n",
    "def plot_confusion_matrix(y_pred, title='clusters'):\n",
    "    cm = confusion_matrix(categorys, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_and_martix(clusters, m):\n",
    "    print_metrics(clusters)\n",
    "    plot_confusion_matrix(clusters, title=m['title'])\n",
    "    new_labels = get_new_labels(clusters)\n",
    "    print_metrics(new_labels)\n",
    "    plot_confusion_matrix(new_labels, title=m['title'])\n",
    "    \n",
    "def analyse(m, data):\n",
    "    model = m['model']\n",
    "    print('\\n' + m['title'] + \" results\")\n",
    "    dataset = data[\"matrix\"]\n",
    "    if m['title'] == KMtitle:\n",
    "        clusters = model.fit_predict(dataset)\n",
    "        print(np.unique(clusters, return_counts=True)[1])\n",
    "        \n",
    "        print_top_terms(model, data[\"names\"])\n",
    "        metrics_and_martix(clusters, m)\n",
    "        \n",
    "    if m['title'] == EMtitle:\n",
    "        model.fit(dataset.toarray())\n",
    "        clusters = model.predict(dataset.toarray())\n",
    "        print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        print_top_terms(model, data[\"names\"])\n",
    "        metrics_and_martix(clusters, m)\n",
    "        \n",
    "    if m['title'] in [ACtitle, AAtitle, AWtitle]:\n",
    "        clusters = model.fit_predict(dataset.toarray())\n",
    "        print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        metrics_and_martix(clusters, m)\n",
    "        \n",
    "    if m['title'] == DBSCANtitle:\n",
    "        for e in [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]:\n",
    "            for m in [3, 4, 5, 6, 7, 8]:\n",
    "                model.set_params(eps = e, min_samples = m,)\n",
    "                clusters = model.fit_predict(dataset)\n",
    "                \n",
    "                results = np.unique(clusters, return_counts=True)\n",
    "                if results[0][0] == -1: #if there was noise \n",
    "                    n_noise    = results[1][0]\n",
    "                    n_clusters = np.sort(results[1][1:])[::-1]\n",
    "                else:\n",
    "                    n_noise    = 0      #if there was no noise \n",
    "                    n_clusters = np.sort(results[1])[::-1]\n",
    "                print (\"ε=%.1f min=%i: noise=%4i clusters=%3i top10=%s\" \n",
    "                       %(e, m, n_noise, len(n_clusters), n_clusters[:10]))\n",
    "    else:\n",
    "        print(m)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">(4058, 18118)\n",
      "\n",
      "K-means results\n",
      "[ 459 1262  372  588 1377]\n",
      "Cluster 0:  spo spor mpio čemp pion empi  čem port varž nkty\n",
      "Cluster 1: inti gas  film dain sti   dai nti   žmo ilma  fil\n",
      "Cluster 2: gtyn ngty  run rung vart tynė ynės nės  įvar ungt\n",
      "Cluster 3: omob utom mobi obil tomo ilis bili auto  aut vair\n",
      "Cluster 4: cija  pro toja ojas darb eura monė  eur otoj  dar\n",
      "Rand        0.415\n",
      "Homogeneity 0.512\n",
      "Completeness 0.558\n",
      "[[122  32   0 562 179]\n",
      " [  3 740   0   1  35]\n",
      " [331  38 372   1  18]\n",
      " [  2 420   0   4 411]\n",
      " [  1  32   0  20 734]]\n",
      "New labels:\n",
      "0 -> 2(Sportas)\n",
      "1 -> 1(Veidai)\n",
      "2 -> 2(Sportas)\n",
      "3 -> 0(Auto)\n",
      "4 -> 4(Verslas)\n",
      "[ 588 1262  831    0 1377]\n",
      "Rand        0.460\n",
      "Homogeneity 0.499\n",
      "Completeness 0.601\n",
      "[[562  32 122   0 179]\n",
      " [  1 740   3   0  35]\n",
      " [  1  38 703   0  18]\n",
      " [  4 420   2   0 411]\n",
      " [ 20  32   1   0 734]]\n",
      "{'title': 'K-means', 'model': KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=5, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)}\n",
      "> K-means 413.0453910827637 to run\n",
      "\n",
      "Expectation–maximization results\n",
      "(array([0, 1, 2, 3, 4]), array([ 422,  747, 1403,  698,  788]))\n",
      "Cluster 0: moks oksl  mok ksli slin kosm  tyr  kos lini  gal\n",
      "Cluster 1: dain film ilma  dai  fil lmas uzik muzi  muz inti\n",
      "Cluster 2: cija  pro toja ojas darb  dar etai eura ybė   eur\n",
      "Cluster 3: omob mobi obil utom tomo ilis bili auto  aut vair\n",
      "Cluster 4: ynės nės  tynė mpio čemp pion empi  čem rung  run\n",
      "Rand        0.542\n",
      "Homogeneity 0.598\n",
      "Completeness 0.627\n",
      "[[ 13   6 127 646 103]\n",
      " [  4 705  66   1   3]\n",
      " [  4  11  61   4 680]\n",
      " [390  17 409  19   2]\n",
      " [ 11   8 740  28   0]]\n",
      "New labels:\n",
      "0 -> 3(Mokslas)\n",
      "1 -> 1(Veidai)\n",
      "2 -> 4(Verslas)\n",
      "3 -> 0(Auto)\n",
      "4 -> 2(Sportas)\n",
      "[ 698  747  788  422 1403]\n",
      "Rand        0.542\n",
      "Homogeneity 0.598\n",
      "Completeness 0.627\n",
      "[[646   6 103  13 127]\n",
      " [  1 705   3   4  66]\n",
      " [  4  11 680   4  61]\n",
      " [ 19  17   2 390 409]\n",
      " [ 28   8   0  11 740]]\n",
      "{'title': 'Expectation–maximization', 'model': GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
      "        means_init=None, n_components=5, n_init=1, precisions_init=None,\n",
      "        random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,\n",
      "        verbose_interval=10, warm_start=False, weights_init=None)}\n",
      "> Expectation–maximization 94.62214756011963 to run\n",
      "\n",
      "Complete-linkage clustering results\n",
      "(array([0, 1, 2, 3, 4]), array([ 717, 2608,  633,   27,   73]))\n",
      "Rand        0.134\n",
      "Homogeneity 0.214\n",
      "Completeness 0.348\n",
      "[[ 39 778  77   1   0]\n",
      " [ 89 672  11   7   0]\n",
      " [ 15 130 542   0  73]\n",
      " [289 543   0   5   0]\n",
      " [285 485   3  14   0]]\n",
      "New labels:\n",
      "0 -> 3(Mokslas)\n",
      "1 -> 0(Auto)\n",
      "2 -> 2(Sportas)\n",
      "3 -> 4(Verslas)\n",
      "4 -> 2(Sportas)\n",
      "[2608    0  706  717   27]\n",
      "Rand        0.148\n",
      "Homogeneity 0.212\n",
      "Completeness 0.367\n",
      "[[778   0  77  39   1]\n",
      " [672   0  11  89   7]\n",
      " [130   0 615  15   0]\n",
      " [543   0   0 289   5]\n",
      " [485   0   3 285  14]]\n",
      "{'title': 'Complete-linkage clustering', 'model': AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
      "            connectivity=None, linkage='complete', memory=None,\n",
      "            n_clusters=5, pooling_func='deprecated')}\n",
      "> Complete-linkage clustering 145.9508135318756 to run\n",
      "\n",
      "Average-linkage clustering results\n",
      "(array([0, 1, 2, 3, 4]), array([4051,    3,    1,    1,    2]))\n",
      "Rand        0.000\n",
      "Homogeneity 0.002\n",
      "Completeness 0.186\n",
      "[[895   0   0   0   0]\n",
      " [779   0   0   0   0]\n",
      " [758   0   0   0   2]\n",
      " [832   3   1   1   0]\n",
      " [787   0   0   0   0]]\n",
      "New labels:\n",
      "0 -> 0(Auto)\n",
      "1 -> 3(Mokslas)\n",
      "2 -> 3(Mokslas)\n",
      "3 -> 3(Mokslas)\n",
      "4 -> 2(Sportas)\n",
      "[4051    0    2    5]\n",
      "Rand        0.000\n",
      "Homogeneity 0.002\n",
      "Completeness 0.202\n",
      "[[895   0   0   0   0]\n",
      " [779   0   0   0   0]\n",
      " [758   0   2   0   0]\n",
      " [832   0   0   5   0]\n",
      " [787   0   0   0   0]]\n",
      "{'title': 'Average-linkage clustering', 'model': AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
      "            connectivity=None, linkage='average', memory=None,\n",
      "            n_clusters=5, pooling_func='deprecated')}\n",
      "> Average-linkage clustering 127.54400825500488 to run\n",
      "\n",
      "Ward-linkage clustering results\n",
      "(array([0, 1, 2, 3, 4]), array([2236,  300,  649,  400,  473]))\n",
      "Rand        0.313\n",
      "Homogeneity 0.471\n",
      "Completeness 0.586\n",
      "[[127   1 613   0 154]\n",
      " [484 291   2   0   2]\n",
      " [ 40   2   2 400 316]\n",
      " [827   2   8   0   0]\n",
      " [758   4  24   0   1]]\n",
      "New labels:\n",
      "0 -> 3(Mokslas)\n",
      "1 -> 1(Veidai)\n",
      "2 -> 0(Auto)\n",
      "3 -> 2(Sportas)\n",
      "4 -> 2(Sportas)\n",
      "[ 649  300  873 2236]\n",
      "Rand        0.353\n",
      "Homogeneity 0.454\n",
      "Completeness 0.638\n",
      "[[613   1 154 127   0]\n",
      " [  2 291   2 484   0]\n",
      " [  2   2 716  40   0]\n",
      " [  8   2   0 827   0]\n",
      " [ 24   4   1 758   0]]\n",
      "{'title': 'Ward-linkage clustering', 'model': AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
      "            connectivity=None, linkage='ward', memory=None, n_clusters=5,\n",
      "            pooling_func='deprecated')}\n",
      "> Ward-linkage clustering 124.24353957176208 to run\n",
      "\n",
      "DBSCAN results\n",
      "ε=0.6 min=3: noise=3981 clusters= 19 top10=[14  6  5  5  4  4  3  3  3  3]\n",
      "ε=0.6 min=4: noise=4021 clusters=  6 top10=[14  6  5  4  4  4]\n",
      "ε=0.6 min=5: noise=4033 clusters=  3 top10=[14  6  5]\n",
      "ε=0.6 min=6: noise=4038 clusters=  2 top10=[14  6]\n",
      "ε=0.6 min=7: noise=4044 clusters=  1 top10=[14]\n",
      "ε=0.6 min=8: noise=4044 clusters=  1 top10=[14]\n",
      "ε=0.7 min=3: noise=3898 clusters= 43 top10=[14  6  6  6  5  5  5  4  4  4]\n",
      "ε=0.7 min=4: noise=3999 clusters= 10 top10=[14  6  6  6  5  5  5  4  4  4]\n",
      "ε=0.7 min=5: noise=4012 clusters=  7 top10=[14  6  6  5  5  5  5]\n",
      "ε=0.7 min=6: noise=4032 clusters=  3 top10=[14  6  6]\n",
      "ε=0.7 min=7: noise=4044 clusters=  1 top10=[14]\n",
      "ε=0.7 min=8: noise=4044 clusters=  1 top10=[14]\n",
      "ε=0.8 min=3: noise=3642 clusters= 86 top10=[22 11 11 11 10  9  9  9  8  8]\n",
      "ε=0.8 min=4: noise=3788 clusters= 44 top10=[22 11 11 11 10  9  9  8  7  7]\n",
      "ε=0.8 min=5: noise=3876 clusters= 26 top10=[22 11 10  9  9  8  8  7  7  6]\n",
      "ε=0.8 min=6: noise=3932 clusters= 15 top10=[22 11 10  9  9  8  8  7  6  6]\n",
      "ε=0.8 min=7: noise=3984 clusters=  7 top10=[22 11  9  9  8  8  7]\n",
      "ε=0.8 min=8: noise=3995 clusters=  6 top10=[19 10  9  9  8  8]\n",
      "ε=0.9 min=3: noise=2964 clusters=134 top10=[119 100  41  35  35  33  23  22  22  15]\n",
      "ε=0.9 min=4: noise=3176 clusters= 78 top10=[99 96 39 35 35 32 22 22 22 15]\n",
      "ε=0.9 min=5: noise=3307 clusters= 59 top10=[93 92 35 35 30 25 22 22 21 15]\n",
      "ε=0.9 min=6: noise=3413 clusters= 46 top10=[65 62 35 35 29 25 25 24 22 21]\n",
      "ε=0.9 min=7: noise=3526 clusters= 33 top10=[59 58 35 27 24 24 24 24 22 21]\n",
      "ε=0.9 min=8: noise=3604 clusters= 24 top10=[57 55 35 26 24 24 23 22 22 21]\n",
      "ε=1.0 min=3: noise=2038 clusters=120 top10=[655 459  93  52  39  35  30  25  22  19]\n",
      "ε=1.0 min=4: noise=2212 clusters= 84 top10=[618 448  83  50  37  35  30  22  20  17]\n",
      "ε=1.0 min=5: noise=2381 clusters= 57 top10=[594 435  83  50  35  35  28  19  18  17]\n",
      "ε=1.0 min=6: noise=2502 clusters= 49 top10=[577 323  84  78  50  35  34  26  19  15]\n",
      "ε=1.0 min=7: noise=2636 clusters= 37 top10=[502 321  72  66  50  33  32  28  26  17]\n",
      "ε=1.0 min=8: noise=2730 clusters= 34 top10=[475 317  67  64  49  30  25  24  23  15]\n",
      "ε=1.1 min=3: noise= 949 clusters= 41 top10=[2939    8    8    7    7    6    6    5    5    5]\n",
      "ε=1.1 min=4: noise=1056 clusters= 30 top10=[2720  133   10    9    7    7    7    7    7    6]\n",
      "ε=1.1 min=5: noise=1166 clusters= 22 top10=[2650  126   10    7    7    7    7    6    6    6]\n",
      "ε=1.1 min=6: noise=1267 clusters= 15 top10=[2579  117   13   11   10    7    7    7    6    6]\n",
      "ε=1.1 min=7: noise=1364 clusters= 11 top10=[2472  117   33   13   13   12   11    8    7    5]\n",
      "ε=1.1 min=8: noise=1433 clusters= 11 top10=[2162  238  113   33   22   13   11    9    8    8]\n",
      "ε=1.2 min=3: noise= 174 clusters=  4 top10=[3872    6    3    3]\n",
      "ε=1.2 min=4: noise= 192 clusters=  4 top10=[3851    6    5    4]\n",
      "ε=1.2 min=5: noise= 210 clusters=  4 top10=[3833    6    5    4]\n",
      "ε=1.2 min=6: noise= 237 clusters=  2 top10=[3815    6]\n",
      "ε=1.2 min=7: noise= 257 clusters=  2 top10=[3797    4]\n",
      "ε=1.2 min=8: noise= 277 clusters=  1 top10=[3781]\n",
      "ε=1.3 min=3: noise=   1 clusters=  1 top10=[4057]\n",
      "ε=1.3 min=4: noise=   2 clusters=  1 top10=[4056]\n",
      "ε=1.3 min=5: noise=   2 clusters=  1 top10=[4056]\n",
      "ε=1.3 min=6: noise=   2 clusters=  1 top10=[4056]\n",
      "ε=1.3 min=7: noise=   2 clusters=  1 top10=[4056]\n",
      "ε=1.3 min=8: noise=   2 clusters=  1 top10=[4056]\n",
      "> DBSCAN 361.8052442073822 to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for dataset in datasets:\n",
    "    print('>' + str(dataset[\"matrix\"].shape))\n",
    "    for method in methods:\n",
    "        start_time = time.time()\n",
    "        analyse(method, dataset)\n",
    "        print (\">\", method['title'], time.time() - start_time, \"to run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
