{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"delfi.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 5233 to 4058.\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = []\n",
    "for d in data:\n",
    "    if d[\"categorys\"] in (\"projektai\", \"m360\") or len(d[\"text\"]) < 1000:# d[\"text\"] == \"\": #len(d[\"text\"]) < 1000:\n",
    "        continue\n",
    "    elif d[\"categorys\"] == 'sportas':\n",
    "        d[\"categorys\"] = 'Sportas'\n",
    "    elif d[\"categorys\"][0].startswith('DELFI '):\n",
    "        d[\"categorys\"] = d[\"categorys\"][0][6:]\n",
    "    elif isinstance(d[\"categorys\"], list):\n",
    "        d[\"categorys\"] = d[\"categorys\"][0]\n",
    "    cleaned_data.append(d)\n",
    "cleaned_data = [d for d in cleaned_data if d[\"categorys\"] in (\"Verslas\", \"Mokslas\", \"Veidai\", \"Auto\", \"Sportas\")]\n",
    "print(\"From {} to {}.\".format(len(data), len(cleaned_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.92 s, sys: 16.2 s, total: 23.1 s\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "num_tok, stop_tokens, num_stems = [], [], []\n",
    "\n",
    "text_file = open(\"Lithuanian stop words\", \"r\")\n",
    "stopwords = text_file.read().split(\"\\n\")\n",
    "\n",
    "for d in cleaned_data:#log_progress(cleaned_data):\n",
    "    \n",
    "    # if intro in bigger then text\n",
    "    if len(d['text']) < len(d['intro']):\n",
    "        print(d['text'] + '\\n' + len(d['intro']))\n",
    "    \n",
    "    # tokenize & lowercase\n",
    "    tokens = re.sub(\"[\\W\\d_]+\", \" \", d[\"text\"]).lower().split() # ka daryti su 1992-ųjų, romėniškais skaičiais, 2 mln. eur\\\n",
    "    num_tok += [len(tokens)]\n",
    "    \n",
    "    # remove stop words\n",
    "    new_tokens = [words for words in tokens if words not in stopwords]\n",
    "    stop_tokens += [len(tokens) - len(new_tokens)]\n",
    "    \n",
    "    # steam\n",
    "    with open(\"tokens.txt\", \"w\") as token_file:\n",
    "        token_file.write(\"\\n\".join(new_tokens))\n",
    "    args = (\"./stemwords\", \"-l\", \"lt\", \"-i\", \"tokens.txt\", \"-o\", \"stems.txt\")\n",
    "    popen = subprocess.Popen(args, stdout=subprocess.PIPE)\n",
    "    popen.wait()\n",
    "    with open(\"stems.txt\", \"r\") as stem_file:\n",
    "        stems = stem_file.read().split(\"\\n\")\n",
    "    \n",
    "    # put into dic\n",
    "    d[\"tokens\"] = new_tokens\n",
    "    d[\"stems\"] = stems\n",
    "    num_stems += [len(stems)]\n",
    "\n",
    "\n",
    "#     print(\"In total tokens: {}, stop words removed: {}, stems: {}\".format(num_tok, stop_tokens, num_stems))\n",
    "# text_file.close()\n",
    "#print(len([s for s in d['stems'] for d in data]))\n",
    "# save as file\n",
    "with open(\"delfi_pre.json\", \"w\") as write_file:\n",
    "    json.dump(cleaned_data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415.8876293740759 97 3335\n",
      "85.43592902907837 2 949\n",
      "331.45170034499756 88 2387\n"
     ]
    }
   ],
   "source": [
    "for num in [num_tok, stop_tokens, num_stems]:\n",
    "    print(\"{} {} {}\".format(np.mean(num), np.min(num), np.max(num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open(\"delfi_pre.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "stems = [d[\"stems\"] for d in data]\n",
    "tokens = [d[\"tokens\"] for d in data]\n",
    "category_names = ['Auto', 'Veidai', 'Sportas', 'Mokslas', 'Verslas']\n",
    "categorys = np.array([category_names.index(d[\"categorys\"]) for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47707\n",
      "141370\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "allstems = list(itertools.chain.from_iterable(stems))\n",
    "alltokens = list(itertools.chain.from_iterable(tokens))\n",
    "# print(stems[1])\n",
    "# print(tokens[1])\n",
    "print (len(set(allstems)))\n",
    "print (len(set(alltokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 56 ms, total: 2.01 s\n",
      "Wall time: 2.07 s\n",
      "(4058, 23790)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=(47581 // 2)) # half of total number of features\n",
    "%time X = vectorizer.fit_transform(stems)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "K = 5\n",
    "jobs = -1\n",
    "\n",
    "KMtitle = \"K-means\"\n",
    "KMmodel = KMeans(n_clusters=K,\n",
    "#                  max_iter=1,\n",
    "#                  n_init=1,\n",
    "                 n_jobs=jobs,\n",
    "                 random_state=42,)\n",
    "\n",
    "EMtitle = \"Expectation–maximization\"\n",
    "EMmodel = GaussianMixture(n_components=K,\n",
    "                        covariance_type='diag',\n",
    "#                         n_init=10,\n",
    "                        random_state=42,)\n",
    "\n",
    "ACtitle = \"Complete-linkage clustering\"\n",
    "ACmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='complete',)\n",
    "AAtitle = \"Average-linkage clustering\"\n",
    "AAmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='average',)\n",
    "AWtitle = \"Ward-linkage clustering\"\n",
    "AWmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='ward',)\n",
    "DBSCANtitle = \"DBSCAN\"\n",
    "DBSCANmodel = DBSCAN(n_jobs = jobs,)\n",
    "\n",
    "models = [\n",
    "          {\"model\": KMmodel, \"title\": KMtitle},\n",
    "          {\"model\": EMmodel, \"title\": EMtitle},\n",
    "          {\"model\": ACmodel, \"title\": ACtitle},\n",
    "          {\"model\": AAmodel, \"title\": AAtitle},\n",
    "          {\"model\": AWmodel, \"title\": AWtitle},\n",
    "          {\"model\": DBSCANmodel, \"title\": DBSCANtitle},\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import mode\n",
    "\n",
    "def get_new_labels(clusters):\n",
    "    new_labels = np.zeros_like(clusters)\n",
    "    print(\"New labels:\")\n",
    "    for i in range(K):\n",
    "        mask = (clusters == i)\n",
    "        closest_category = mode(categorys[mask])[0][0]\n",
    "        new_labels[mask] = closest_category\n",
    "        print(\"{} -> {}({})\".format(i, closest_category, category_names[closest_category]))\n",
    "    print(np.bincount(new_labels))\n",
    "    return new_labels\n",
    "\n",
    "def print_top_terms(model):\n",
    "    print(\"Top terms per cluster:\")\n",
    "    centers = model.cluster_centers_ if isinstance(model, KMeans) else model.means_\n",
    "    order_centroids = centers.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(K):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n",
    "\n",
    "def print_metrics(y_pred):\n",
    "    print(\"Clustering print_metrics:\")\n",
    "    print(\" Rand   Mutual information   Homogeneity   Coompleteness   V-measure    Fowlkes mallows\")\n",
    "    print(\"{0:.3f}                {1:.3f}         {2:.3f}           {3:.3f}       {4:.3f}      {5:.3f}\"\n",
    "      .format(adjusted_rand_score(categorys, y_pred),\n",
    "              adjusted_mutual_info_score(categorys, y_pred),\n",
    "              homogeneity_score(categorys, y_pred),\n",
    "              completeness_score(categorys, y_pred),\n",
    "              v_measure_score(categorys, y_pred),\n",
    "              fowlkes_mallows_score(categorys, y_pred),\n",
    "              ))\n",
    "\n",
    "def plot_confusion_matrix(y_pred, title='clusters'):\n",
    "    cm = confusion_matrix(categorys, y_pred)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix of \" + title)\n",
    "    tick_marks = np.arange(len(category_names))\n",
    "    plt.yticks(tick_marks, category_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # put numbers inside cells\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()\n",
    "    \n",
    "# KMmodel = KMeans(n_clusters=K,\n",
    "#                  max_iter=10,\n",
    "#                  n_init=1,\n",
    "#                  n_jobs=-1,\n",
    "#                  random_state=42,)\n",
    "# type(KMmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DBSCAN results\n",
      "ε=1.1 min=3: noise=2287 clusters=177 top10=[144 138 121  89  88  66  63  49  49  41]\n",
      "New labels:\n",
      "0 -> 0(Auto)\n",
      "1 -> 2(Sportas)\n",
      "2 -> 4(Verslas)\n",
      "3 -> 3(Mokslas)\n",
      "4 -> 4(Verslas)\n",
      "[3808    0  138   14   98]\n",
      "Clustering print_metrics:\n",
      " Rand   Mutual information   Homogeneity   Coompleteness   V-measure    Fowlkes mallows\n",
      "0.008                0.051         0.052           0.296       0.089      0.427\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-40fe77d7d16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 print (\"ε=%.1f min=%i: noise=%4i clusters=%3i top10=%s\" \n\u001b[1;32m     47\u001b[0m                        %(e, m, n_noise, len(n_clusters), n_clusters[:10]))\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mmetrics_and_martix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-40fe77d7d16c>\u001b[0m in \u001b[0;36mmetrics_and_martix\u001b[0;34m(clusters)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_new_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def metrics_and_martix(clusters):\n",
    "    print_metrics(clusters)\n",
    "    plot_confusion_matrix(clusters, title=m['title'])\n",
    "    new_labels = get_new_labels(clusters)\n",
    "    print_metrics(new_labels)\n",
    "    plot_confusion_matrix(new_labels, title=m['title'])\n",
    "\n",
    "for m in models:\n",
    "    model = m['model']\n",
    "    print('\\n' + m['title'] + \" results\")\n",
    "    \n",
    "    if m['title'] == KMtitle:\n",
    "        %time clusters = model.fit_predict(X)\n",
    "        print(np.unique(clusters, return_counts=True)[1])\n",
    "        \n",
    "        print_top_terms(model)\n",
    "        metrics_and_martix(clusters)\n",
    "        \n",
    "    elif m['title'] == EMtitle:\n",
    "        %time model.fit(X.toarray())\n",
    "        clusters = model.predict(X.toarray())\n",
    "        print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        print_top_terms(model)\n",
    "        metrics_and_martix(clusters)\n",
    "        \n",
    "    elif m['title'] in [ACtitle, AAtitle, AWtitle]:\n",
    "        %time clusters = model.fit_predict(X.toarray())\n",
    "        print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        metrics_and_martix(clusters)\n",
    "        \n",
    "    elif m['title'] == DBSCANtitle:\n",
    "        for e in [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]:\n",
    "            for m in [3, 4, 5, 6, 7, 8]:\n",
    "                model.set_params(eps = e, min_samples = m,)\n",
    "                clusters = model.fit_predict(X)\n",
    "                \n",
    "                results = np.unique(clusters, return_counts=True)\n",
    "                if results[0][0] == -1: #if there was noise \n",
    "                    n_noise    = results[1][0]\n",
    "                    n_clusters = np.sort(results[1][1:])[::-1]\n",
    "                else:\n",
    "                    n_noise    = 0      #if there was no noise \n",
    "                    n_clusters = np.sort(results[1])[::-1]\n",
    "                print (\"ε=%.1f min=%i: noise=%4i clusters=%3i top10=%s\" \n",
    "                       %(e, m, n_noise, len(n_clusters), n_clusters[:10]))\n",
    "    else:\n",
    "        print(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
