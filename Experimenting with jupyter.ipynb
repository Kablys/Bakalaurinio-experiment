{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Do document clustering experiment.')\n",
    "parser.add_argument(\"--raw\",      action='store_true')\n",
    "parser.add_argument(\"--title\",    action='store_true')\n",
    "parser.add_argument(\"--intro\",    action='store_true')\n",
    "parser.add_argument(\"--stopword\", action='store_true')\n",
    "parser.add_argument(\"--stemer\",   action='store_true')\n",
    "parser.add_argument(\"--lemstop\",  action='store_true')\n",
    "parser.add_argument(\"--lemmer\",   action='store_true')\n",
    "\n",
    "parser.add_argument(\"--ngram\",  type=int,   choices=range(1, 7),)\n",
    "parser.add_argument(\"--min_df\", type=int,   choices=range(1, 11),)\n",
    "parser.add_argument(\"--max_df\", type=float, choices=(x/20 for x in range(1, 20)),) #0.05, 0.1 ... 0.95\n",
    "parser.add_argument(\"--dr\",     type=int,   choices=range(1, 101),)\n",
    "\n",
    "parser.add_argument('--methods', nargs='+', choices='km em ac aa aw db'.split(),\n",
    "                    default='km em ac aa aw db'.split(),)\n",
    "\n",
    "#args = parser.parse_args() \n",
    "args = parser.parse_args('--dr 10'.split()) # For testing\n",
    "print(\">\" + str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "pre_received, lemma_received = (False, False)\n",
    "data, all_tokens, morfs = ([],[],[])\n",
    "def get_pre_data():\n",
    "    global pre_received, data, all_tokens\n",
    "    if not pre_received:\n",
    "        with open(\"delfi_pre.json\", \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "        all_tokens = [\" \".join(d[\"tokens\"] + d[\"stop_tokens\"]) for d in data]\n",
    "        pre_received = True\n",
    "                        \n",
    "def get_lemma_data():\n",
    "    global lemma_received, morfs\n",
    "    if not lemma_received:\n",
    "        with open(\"delfi_lemmas.json\", \"r\") as read_file:\n",
    "            lemmaData = json.load(read_file)\n",
    "        morfs = [re.findall(\"<word=\\\"(.*)\\\" lemma=\\\"(\\w*).*\\\" type=\\\"(.*)\\\"\", d[\"lemms\"]) for d in lemmaData]\n",
    "        lemma_received = True\n",
    "\n",
    "def make_dataset(experiment_data, **kwargs):\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    matrix = vectorizer.fit_transform(experiment_data)\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print('>' + str(vectorizer))\n",
    "    return [{\"matrix\" : matrix, \"names\" : names}]  \n",
    "\n",
    "get_pre_data() # TODO run these only if needed\n",
    "#get_lemma_data()\n",
    "datasets = []\n",
    "\n",
    "if args.raw:  \n",
    "    datasets += make_dataset(all_tokens)\n",
    "if args.title:\n",
    "    datasets += make_dataset([re.sub(\"[\\W\\d_]+\", \" \", d[\"title\"]).lower() for d in data])\n",
    "if args.intro:\n",
    "    datasets += make_dataset([re.sub(\"[\\W\\d_]+\", \" \", d[\"intro\"]).lower() for d in data])\n",
    "if args.stopword:\n",
    "    datasets += make_dataset([\" \".join(d[\"tokens\"])                    for d in data])\n",
    "if args.stemer:\n",
    "    datasets += make_dataset([\" \".join(d[\"stems\"]  + d[\"stop_stems\"])  for d in data])\n",
    "\n",
    "if args.lemstop:\n",
    "    datasets += make_dataset([\" \".join([l[0] for l in m if l[2].startswith((\"dkt\", \"vksm\", \"bdv\"))]) for m in morfs])\n",
    "if args.lemmer:\n",
    "    datasets += make_dataset([\" \".join([l[1] for l in m]) for m in morfs])\n",
    "    \n",
    "if args.ngram:\n",
    "    n = args.ngram\n",
    "    datasets += make_dataset(all_tokens, analyzer = 'char_wb', ngram_range = (n,n))\n",
    "if args.min_df:\n",
    "    df = args.min_df\n",
    "    datasets += make_dataset(all_tokens, min_df = df)\n",
    "if args.max_df:\n",
    "    df = args.min_df\n",
    "    datasets += make_dataset(all_tokens, max_df = df)\n",
    "if args.dr:\n",
    "    svd = TruncatedSVD(args.dr)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False)) \n",
    "    X = make_dataset(all_tokens)[0]\n",
    "    matrix = csr_matrix(lsa.fit_transform(X[\"matrix\"]))\n",
    "    datasets += [{\"matrix\" : matrix, \"names\" : X[\"names\"]}]\n",
    "\n",
    "#print('>' + str(datasets[0][\"matrix\"]))\n",
    "\n",
    "category_names = ['Auto', 'Veidai', 'Sportas', 'Mokslas', 'Verslas']\n",
    "categorys  = np.array([category_names.index(d[\"categorys\"]) for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "K = 5\n",
    "jobs = -1\n",
    "\n",
    "methods = []\n",
    "\n",
    "if \"km\" in args.methods:\n",
    "    KMtitle = \"K-means\"\n",
    "    KMmodel = KMeans(n_clusters=K,\n",
    "#                  max_iter=1,\n",
    "#                  n_init=1,\n",
    "                 n_jobs=jobs,\n",
    "                 random_state=42,)\n",
    "    methods += [{\"model\": KMmodel, \"title\": KMtitle}]\n",
    "    \n",
    "if \"em\" in args.methods:\n",
    "    EMtitle = \"Expectation–maximization\"\n",
    "    EMmodel = GaussianMixture(n_components=K,\n",
    "                              covariance_type='diag',\n",
    "#                         n_init=10,\n",
    "                              random_state=42,)\n",
    "    methods += [{\"model\": EMmodel, \"title\": EMtitle}]\n",
    "    \n",
    "if \"ac\" in args.methods:\n",
    "    ACtitle = \"Complete-linkage clustering\"\n",
    "    ACmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                      linkage='complete',)\n",
    "    methods += [{\"model\": ACmodel, \"title\": ACtitle}]\n",
    "    \n",
    "if \"aa\" in args.methods:\n",
    "    AAtitle = \"Average-linkage clustering\"\n",
    "    AAmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='average',)\n",
    "    methods += [{\"model\": AAmodel, \"title\": AAtitle}]\n",
    "    \n",
    "if \"aw\" in args.methods:\n",
    "    AWtitle = \"Ward-linkage clustering\"\n",
    "    AWmodel = AgglomerativeClustering(n_clusters=K,\n",
    "                                  linkage='ward',)\n",
    "    methods += [{\"model\": AWmodel, \"title\": AWtitle}]\n",
    "    \n",
    "if \"db\" in args.methods:\n",
    "    DBSCANtitle = \"DBSCAN\"\n",
    "    DBSCANmodel = DBSCAN(n_jobs = jobs,)\n",
    "    methods += [{\"model\": DBSCANmodel, \"title\": DBSCANtitle}]\n",
    "print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import mode\n",
    "\n",
    "def print_top_terms(model, terms):\n",
    "#    print(\"Top terms per cluster:\")\n",
    "    centers = model.cluster_centers_ if isinstance(model, KMeans) else model.means_\n",
    "    order_centroids = centers.argsort()[:, ::-1]\n",
    "    for i in range(K):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n",
    "\n",
    "def get_new_labels(clusters):\n",
    "    new_labels = np.zeros_like(clusters)\n",
    "    print(\"New labels:\")\n",
    "    for i in range(K):\n",
    "        mask = (clusters == i)\n",
    "        closest_category = mode(categorys[mask])[0][0]\n",
    "        new_labels[mask] = closest_category\n",
    "        print(\"{} -> {}({})\".format(i, closest_category, category_names[closest_category]))\n",
    "    print(np.bincount(new_labels))\n",
    "    return new_labels\n",
    "\n",
    "def print_metrics(y_pred):\n",
    "    print(\"Rand        %.3f\" %(adjusted_rand_score(categorys, y_pred)))\n",
    "    print(\"Homogeneity %.3f\" %(homogeneity_score(categorys, y_pred)))\n",
    "    print(\"Homogeneity %.3f\" %(completeness_score(categorys, y_pred)))\n",
    "\n",
    "def plot_confusion_matrix(y_pred, title='clusters'):\n",
    "    cm = confusion_matrix(categorys, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_and_martix(clusters, m):\n",
    "    print_metrics(clusters)\n",
    "    plot_confusion_matrix(clusters, title=m['title'])\n",
    "    new_labels = get_new_labels(clusters)\n",
    "    print_metrics(new_labels)\n",
    "    plot_confusion_matrix(new_labels, title=m['title'])\n",
    "    \n",
    "def analyse(m, data):\n",
    "    model = m['model']\n",
    "    print('\\n' + m['title'] + \" results\")\n",
    "    dataset = data[\"matrix\"]\n",
    "    if m['title'] == KMtitle:\n",
    "        clusters = model.fit_predict(dataset)\n",
    "        print(np.unique(clusters, return_counts=True)[1])\n",
    "        \n",
    "        print_top_terms(model, data[\"names\"])\n",
    "        metrics_and_martix(clusters, m)\n",
    "        \n",
    "    if m['title'] == EMtitle:\n",
    "        model.fit(dataset.toarray())\n",
    "        clusters = model.predict(dataset.toarray())\n",
    "        print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        print_top_terms(model, data[\"names\"])\n",
    "        metrics_and_martix(clusters, m)\n",
    "        \n",
    "    if m['title'] in [ACtitle, AAtitle, AWtitle]:\n",
    "        clusters = model.fit_predict(dataset.toarray())\n",
    "        print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        metrics_and_martix(clusters, m)\n",
    "        \n",
    "    if m['title'] == DBSCANtitle:\n",
    "        for e in [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]:\n",
    "            for m in [3, 4, 5, 6, 7, 8]:\n",
    "                model.set_params(eps = e, min_samples = m,)\n",
    "                clusters = model.fit_predict(dataset)\n",
    "                \n",
    "                results = np.unique(clusters, return_counts=True)\n",
    "                if results[0][0] == -1: #if there was noise \n",
    "                    n_noise    = results[1][0]\n",
    "                    n_clusters = np.sort(results[1][1:])[::-1]\n",
    "                else:\n",
    "                    n_noise    = 0      #if there was no noise \n",
    "                    n_clusters = np.sort(results[1])[::-1]\n",
    "                print (\"ε=%.1f min=%i: noise=%4i clusters=%3i top10=%s\" \n",
    "                       %(e, m, n_noise, len(n_clusters), n_clusters[:10]))\n",
    "    else:\n",
    "        print(m)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for dataset in datasets:\n",
    "    print('>' + str(dataset[\"matrix\"].shape))\n",
    "    for method in methods:\n",
    "        start_time = time.time()\n",
    "        analyse(method, dataset)\n",
    "        print (\">\", method['title'], time.time() - start_time, \"to run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
